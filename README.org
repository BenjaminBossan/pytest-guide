#+title: Pytest Guide

* Introduction
The purpose of this guide is to help you get started with ~pytest~ when you are already familiar with ~unittest~. We will learn about the main differences between the two libraries, what the main features of ~pytest~ are, and how to use them well.

This guide is not a comprehensive tutorial on ~pytest~. For this, please refer to the [[https://docs.pytest.org/][official documenation]]. It also contains several opinions, which may or may not apply to your specific use case.

The examples in this guide are accompanied by the tests inside the ~code~ directory. You can run the tests by executing the ~pytest~ command, e.g. ~pytest code/test_unittest_assert.py~. Running the tests requires to install the dependencies: ~python -m pip install -r requirements.txt~.
* Main differences with unittest at a glance
** Error messages
unittest requires specific ~assert~ methods like ~self.assertEqual~ to provide good error feedback. With ~pytest~, normal ~assert~ statements return good error messages. This makes code more readable and less verbose.

In the example below, the first test uses unittest and a plain ~assert~ statement. The error message is not helpful, as it doesn't indicate what exactly went wrong. The second test uses ~unittest~ and ~self.assertEqual~. The error message is better, as it shows the expected and actual values. The third test uses ~pytest~ and a plain ~assert~ statement. The error message is as good as the second test, but the code is more concise.
#+begin_src python
# code/test_unittest_assert.py
class TestUnittestAssert(unittest.TestCase):
    def test_upper(self):
        assert "foo".upper() == "BAR"
# AssertionError

class TestUnittestAssertMethod(unittest.TestCase):
    def test_upper(self):
        self.assertEqual("foo".upper(), "BAR")
# E       AssertionError: 'FOO' != 'BAR'
# E       - FOO
# E       + BAR

class TestPytestAssert:
    def test_upper(self):
        assert "foo".upper() == "BAR"
# E       AssertionError: 'FOO' != 'BAR'
# E       - FOO
# E       + BAR

if __name__ == "__main__":
    unittest.main()
#+end_src

*Note*: Pytest achieves this by changing how the ~assert~ statement works. Therefore, even if you use unittest style tests, you will benefit from the nice error messages when using ~assert~ as long as you use ~pytest~ as the test runner.
** Test classes
In ~unittest~, test classes must inherit from ~unittest.TestCase~. In ~pytest~, they don't need to inherit from anything. You can even write tests as standalone functions, but I recommend to still group them in classes (this is explained in a later section).
#+begin_src python
# code/test_unittest_class.py

# unittest style
class TestLists(unittest.TestCase):
    def test_contains(self):
        self.assertIn(2, [1, 2, 3])

    def test_append(self):
        lst = [1, 2, 3]
        lst.append(4)
        self.assertEqual(lst, [1, 2, 3, 4])

# pytest style using standalone functions
def test_contains():
    assert 2 in [1, 2, 3]

def test_append():
    lst = [1, 2, 3]
    lst.append(4)
    assert lst == [1, 2, 3, 4]
#+end_src
* Fixtures
** Introduction
Fixtures are like assets that can be reused across different tests, allowing you to write efficient and readable tests. In addition, similar to ~unittest~'s ~setUp~ and ~tearDown~ methods, fixtures can be used to set up and tear down resources before and after tests. However, fixtures can also be a bit "magic", so use them with care and ensure that the resulting tests can be easily understood by others.
** Using fixtures in pytest
In ~unittest~, to prepare a resource for a test, you need to define a ~setUp~ method in a test class and store the resource as an attribute on the class, so that you can access it via ~self~ in the test method. In ~pytest~, you need to define a function or method using the ~@pytest.fixture~ decorator. You can then use the fixture in a test function by adding an argument to the function with the same name as the fixture. This is a bit "magic", but it's also more flexible than ~unittest~'s approach.
#+begin_src python
# code/test_unittest_setup.py
from transformers import AutoModelForCausalLM

class TestBloomzUnittest(unittest.TestCase):
    def setUp(self):
        self.model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

    def test_architecture(self):
        self.assertIn("BloomForCausalLM", self.model.config.architectures)

    def test_num_layers(self):
        self.assertEqual(len(self.model.transformer.h), 24)

class TestBloomzPytest:
    @pytest.fixture
    def model(self):
        return AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

    def test_architecture(self, model):
        # fixture is passed as an argument because the name "model" matches the fixture name
        assert "BloomForCausalLM" in model.config.architectures

    def test_num_layers(self, model):
        assert len(model.transformer.h) == 24
#+end_src
As you can see, the ~model~ fixture is automatically passed to the test function because the argument name matches the name of the fixture. This is how ~pytest~ fixtures work. This can be done with as many fixtures as you want.

*Note*: It is not possible to combine fixtures with ~unittest.TestCase~. Therefore, if you want to use fixtures, use a normal class or standalone functions.
** Setup and teardown
In ~unittest~, you need to define ~setUp~ and ~tearDown~ methods in a test class. These methods are called before and after each test method, respectively. In ~pytest~, setup and teardown can be implemented in a fixture by using the ~yield~ statement. The code before the ~yield~ statement is the setup code, and the code after the ~yield~ statement is the teardown code.
#+begin_src python
# code/test_unittest_setup.py
class TestBloomzUnittest(unittest.TestCase):
    def setUp(self):
        self.model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

    def tearDown(self):
        del self.model
        gc.collect()
        torch.cuda.empty_cache()

    def test_architecture(self):
        self.assertIn("BloomForCausalLM", self.model.config.architectures)

class TestBloomzPytest:
    @pytest.fixture
    def model(self):
        # everything until the yield statement is the setup
        yield AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

        # everything after the yield statement is the teardown
        gc.collect()
        torch.cuda.empty_cache()

    def test_architecture(self, model):
        # fixture is passed as an argument because the name "model" matches the fixture name
        assert "BloomForCausalLM" in model.config.architectures
#+end_src
** Nested fixtures
Fixtures can take other fixtures as arguments. This is useful when you want to reuse a fixture in another fixture. ~pytest~ will automatically figure out the dependency graph and create the necessary fixtures.
#+begin_src python
# code/test_unittest_setup.py
class TestBloomzUnittest(unittest.TestCase):
    def setUp(self):
        self.model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")
        self.config = self.model.config

    def test_architecture(self):
        self.assertIn("BloomForCausalLM", self.config.architectures)

class TestBloomzPytest:
    @pytest.fixture
    def model(self):
        return AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

    @pytest.fixture
    def config(self, model):
        # the config fixture uses the model fixture
        return model.config

    def test_architecture(self, config):
        # the config fixture is passed as an argument; as it depends on the
        # model fixture, the model fixture is also called
        assert "BloomForCausalLM" in config.architectures
#+end_src
** Autouse fixtures
~pytest.fixture~ has an argument called ~autouse~, which makes it so that the corresponding fixture is automatically used in all tests. This can be very confusing because when reading the test code, you can't seee that the fixture will be invoked. Therefore, it is recommended to use ~autouse~ extremely sparingly, ideally not at all.

Typically, ~autouse~ fixtures are used when you are only interested in the side effect of the fixture but the fixture doesn't return a useful value. In that case, it can be awkward to include the fixture as an argument to the test function, as the argument is unused. A better solution is to use ~@pytest.mark.usefixtures~ to apply the fixture to the test function without adding it as an argument.
#+begin_src python
# code/test_autouse.py
class TestWithAutouse:
    # BAD: This example uses autouse, which can be confusing.
    @pytest.fixture(autouse=True)
    def no_grad_context(self):
        # this fixture makes it so that if a function uses it, it is automatically called
        # using the torch.no_grad() context
        with torch.no_grad():
            yield

    def test_forward(self):
        model = torch.nn.Linear(3, 5)
        x = torch.randn(10, 3)
        model(x)

class TestFixtureArgument:
    # BETTER: This example does not use autouse. The no_grad_context fixture is
    # passed as an argument but unused, which can be confusing as well.
    @pytest.fixture
    def no_grad_context(self):
        with torch.no_grad():
            yield

    def test_forward(self, no_grad_context):
        model = torch.nn.Linear(3, 5)
        x = torch.randn(10, 3)
        model(x)

class TestWithUsefixture:
    # BEST: This example uses usefixtures, which makes it clear that the fixture
    # is used for its side effect and not for its return value.
    @pytest.fixture
    def no_grad_context(self):
        with torch.no_grad():
            yield

    @pytest.mark.usefixtures("no_grad_context")
    def test_forward(self):
        model = torch.nn.Linear(3, 5)
        x = torch.randn(10, 3)
        model(x)
#+end_src
** Fixture scope
By default, fixtures are "function scoped", i.e. for each function, the fixture is created anew. In general, this is a good thing: When the fixture is re-created for each test, there is no danger of the value being mutated and thus affecting subsequent tests. However, when creating the fixture is expensive, we would prefer to create it only once. This can be achieved by passing the ~scope~ argument when creating the fixture.
#+begin_src python
# code/test_fixture_scope.py
class TestFunctionScope:
    @pytest.fixture
    def lst(self):
        # by default, fixtures are function scoped
        return [1, 2, 3]

    def test_append(self, lst):
        lst.append(4)
        assert lst == [1, 2, 3, 4]

    def test_remove(self, lst):
        # Note: This test would fail if the lst fixture were not re-created for
        # each test.
        lst.remove(2)
        assert lst == [1, 3]

class TestClassScope:
    @pytest.fixture(scope="class")
    def model(self):
        # This fixture is created only once for the entire class. This is good
        # because loading the model is expensive.
        # WARNING: If the model is mutated in any of the test, subsequent tests
        # will be affected.
        return AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

    def test_architecture(self, model):
        assert "BloomForCausalLM" in model.config.architectures

    def test_num_layers(self, model):
        assert len(model.transformer.h) == 24

    def test_mutate_model(self, model):
        # DO NOT DO THIS, as it affects other tests that use the model fixture
        def fine_tune(model):
            # model training code here
            pass

        fine_tune(model)
#+end_src
*Note*: The available scopes are:
- ~function~ (default): The fixture is re-created for each test function.
- ~class~: The fixture is created once for the entire class.
- ~module~: The fixture is created once for the entire module (py file).
- ~package~: The fixture is created once for the entire package.
- ~session~: The fixture is created once for the entire pytest test run.
* Builtin fixtures
** Introduction
~pytest~ provides a number of builtin fixtures that can be very usefu. These fixtures are automatically available in all tests, just use them as arguments to your test functions. There is a [[https://docs.pytest.org/en/latest/reference/fixtures.html][list]] of all builtin fixtures in the official documentation. Here, we will focus on a couple of the most useful fixtures.
** Logging: ~caplog~
To test logging calls, make use of the [[https://docs.pytest.org/en/latest/reference/reference.html#std-fixture-caplog][caplog]] fixture. This fixture captures all log messages and makes them available to the test function. You can then use ~caplog~ to check if the expected log messages were emitted.
#+begin_src python
# code/test_caplog.py
def test_logging(caplog):
    logging.info("This is an info message")
    logging.warning("This is a warning message")
    logging.error("This is an error message")
    # "info" is not recorded by default because the default level is "warning"
    assert len(caplog.records) == 2
    assert caplog.records[0].levelname == "WARNING"
    assert caplog.records[1].levelname == "ERROR"
#+end_src
** Capturing print output: ~capsys~
To test print statements, make use of the [[https://docs.pytest.org/en/latest/reference/reference.html#std-fixture-capsys][capsys]] fixture. This fixture captures everything printed to stdout and stderr.
#+begin_src python
# code/test_capsys.py
def test_print(capsys):
    print("This is printed to stdout")
    print("This is printed to stderr", file=sys.stderr)
    captured = capsys.readouterr()
    assert captured.out == "This is printed to stdout\n"
    assert captured.err == "This is printed to stderr\n"
#+end_src
** Temporary files: ~tmp_path~
When working with temporary files, it is best to use the [[https://docs.pytest.org/en/latest/how-to/tmp_path.html#tmp-path][tmp_path]] fixture. This fixture creates a temporary directory and returns a ~pathlib.Path~ object pointing to it. The temporary directory is automatically cleaned up when the test finishes, only leaving the last 3 temporary directories in case you want to inspect them.
#+begin_src python
# code/test_tmp_path.py
class TestTmpPath:
    @pytest.fixture
    def model(self):
        model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")
        return model

    def test_save(self, model, tmp_path):
        model_dir = tmp_path / "bloomz"
        model.save_pretrained(model_dir)
        assert (model_dir / "config.json").is_file()
        assert (model_dir / "model.safetensors").is_file()
#+end_src
* Testing for exceptions and warnings
** Exceptions: ~pytest.raises~
In ~unittest~, you can use the ~assertRaises~ context manager to test if a function raises an exception. In ~pytest~, use [[https://docs.pytest.org/en/8.0.x/reference/reference.html#pytest-raises][pytest.raises]] instead to achieve the same outcome.
#+begin_src python
# code/test_raises.py
def test_zero_division():
    with pytest.raises(ZeroDivisionError):
        1 / 0

def test_with_message():
    def check_value(value):
        if value <= 10:
            raise ValueError(f"value must be greater than 10, got {value} instead")

    check_value(11)  # works
    with pytest.raises(ValueError, match="value must be greater than 10, got 5 instead"):
        check_value(5)  # raises
#+end_src
*Tip*: The ~match~ argument is a regular expression that is matched against the exception message. The test passes only if the message matches. In most circumstances, you should use ~match~ to ensure that the correct exception is raised, otherwise, an unrelated exception that happens to have the same type will also lead to a passing test.
** Warnings: ~pytest.warns~
To test if a function emits a warning, use the [[https://docs.pytest.org/en/8.0.x/how-to/capture-warnings.html#warns][pytest.warns]] context manager. This basically works the same as ~pytest.raises~ but instead of passing an exception type, you pass a warning type. The ~match~ argument also works the same as with ~pytest.raises~.
#+begin_src python
# code/test_warns.py
def test_warning():
    def warn():
        warnings.warn("This is a warning")

    with pytest.warns(UserWarning):
        warn()

    def another_warn():
        warnings.warn("This is another warning", FutureWarning)

    with pytest.warns(FutureWarning, match="This is another warning"):
        another_warn()
#+end_src
If you want to check that multiple warnings are emitted, you can use the [[https://docs.pytest.org/en/latest/reference/reference.html#std-fixture-recwarn][recwarn]] fixture instead.
* Parametrization
** Introduction
The concept of [[https://docs.pytest.org/en/latest/how-to/parametrize.html][parametrization]] is useful when you want to run the same test with different inputs. This is an excellent way to avoid code duplication and to ensure that the same test is run with different inputs.

Currently, many HF repos use the [[https://github.com/wolever/parameterized][parameterized]] package to achieve this, but with ~pytest~, this is unnecessary because ~pytest~ has built-in support for parametrization: ~pytest.mark.parametrize~.
** Parametrizing test functions
The syntax for ~@pytest.mark.parametrize~ is a bit unusual. As a first argument, it takes a string with the name of the parameter. The same name should be used as an argument to the test function. As a second argument, it takes a sequence of values. The test function will be run once for each value.
#+begin_src python
# code/test_parametrize.py
class TestModel:
    @pytest.mark.parametrize("model_name", ["bigscience/bloomz-560m", "gpt2"])
    def test_forward(self, model_name):
        # this test runs twice, once for each model
        model = AutoModelForCausalLM.from_pretrained(model_name)
        x = torch.zeros(1, 10, dtype=torch.long)
        model(x)
#+end_src
** Multiple arguments in parametrize
It is possible to parametrize multiple arguments at once. In this case, the first argument is a string with the names of the parameters separated by commas. The second argument is a sequence of tuples, where each tuple contains the values for the arguments.
#+begin_src python
# code/test_parametrize.py
class TestModel:
    @pytest.mark.parametrize("model_name, num_layers", [
        ("bigscience/bloomz-560m", 24),
        ("gpt2", 12),
    ])
    def test_layers(self, model_name, num_layers):
        model = AutoModelForCausalLM.from_pretrained(model_name)
        assert len(model.transformer.h) == num_layers
#+end_src
** Nested parametrization
It is possible to add multiple ~@pytest.mark.parametrize~ decorators to a single test function. In this case, the test function will be run once for each combination of the parameters.
#+begin_src python
# code/test_parametrize.py
class TestModel:
    @pytest.mark.parametrize("model_name", ["bigscience/bloomz-560m", "gpt2"])
    @pytest.mark.parametrize("method", ["forward", "generate"])
    def test_has_method(self, model_name, method):
        # this test runs four times, once for each model and method
        model = AutoModelForCausalLM.from_pretrained(model_name)
        assert hasattr(model, method)
#+end_src
** Parametrizing fixtures: It's tricky
Looking at the examples from the previous section, we can spot an issue, namely that we need to call ~AutoModelForCausalLM.from_pretrained~ for each method that we want to test. This is bad as the call is expensive and unnecessary. Normally, we would use a fixture to create the model, but we cannot do this here because we cannot use fixtures inside ~@pytest.mark.parametrize~. However, it is possible to [[https://docs.pytest.org/en/8.0.x/how-to/fixtures.html#parametrizing-fixtures][parametrize fixtures]]. Here is one way:
#+begin_src python
# code/test_parametrize.py
class TestModel2:
    # here we need to use the "request" fixture, which is a builtin fixture from
    # pytest
    @pytest.fixture(scope="class")
    def model(self, request):
        return AutoModelForCausalLM.from_pretrained(request.param)

    # here we use the "model" fixture and parametrize it with the indirect=True
    # argument; note that the "method" parameter has to come first
    @pytest.mark.parametrize("method", ["forward", "generate"])
    @pytest.mark.parametrize("model", ["bigscience/bloomz-560m", "gpt2"], indirect=True)
    def test_with_parametrized_fixture(self, model, method):
        assert hasattr(model, method)
#+end_src
In this test, the ~AutoModelForCausalLM.from_pretrained~ is called only once for each model, making the test more efficient. However, as you can see, this makes the code more complex. Therefore, it is recommended to *avoid* this feature if possible. Only use it if there is no easier way to achieve the same outcome and document it really well.
* Markers
** Introduction
In ~pytest~, the concept of the marker is used to add metadata to tests. This metadata can be used to control the test run, to filter tests, or to add custom behavior to tests. It's possible to define your own markers but most of the time, you will probably use one of the built-in markers.
** Tests that are expected to fail: ~xfail~
In some situations, we want to add a test even though we expect it to fail. For example, this could be a test that fails right now but should pass in the future (say, when a new version of a dependency is released, or when a known bug will be fixed). To mark such tests, use the [[https://docs.pytest.org/en/8.0.x/how-to/skipping.html#xfail-mark-test-functions-as-expected-to-fail][pytest.mark.xfail]] decorator.
#+begin_src python
# code/test_xfail.py
@pytest.mark.xfail
def test_model_can_fly():
    model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")
    assert model.can_fly()  # this method does not exist yet
#+end_src
When running pytest, this test will be symbolized by a yellow ~x~ instead of a green ~.~. This indicates that the test is has x-failed. If the test passes despite being marked as x-failed, it will be symbolized by a yellow capital ~X~ instead. In both cases, the test suite counts as passing (i.e. the exit code is 0).
** Useful ~xfail~ arguments
When a test marked with ~xfail~ does not fail, the test is considered to be valid ("XPASS"). Often, we want the test to fail in that case. To achieve this, pass the argument ~strict=True~ to ~xfail~. This will make the test count as failing if it passes. This way, our CI will alert us to tests suddenly passing when they're expected to fail and we can investigate why.

Another useful argument is ~reason~, which allows you to add a message to the test. This message will be printed when the test fails, which can be useful to explain why the test is expected to fail. Additionally, there is also the ~condition~ argument, which allows you to add a condition that must be met for the test to be marked as x-failed. This can be used for instance to mark tests to fail only on a specific platform or with specific dependency versions.
#+begin_src python
# code/test_xfail.py
@pytest.mark.xfail(reason="Flying is not implemented yet", strict=True)
def test_model_can_fly():
    model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")
    assert model.can_fly()  # this method does not exist yet

@pytest.mark.xfail(reason="This test fails on Windows", condition=sys.platform == "win32")
def test_fail_on_windows():
    assert os.path.exists("/tmp")
#+end_src
** skipping tests: ~skip~
Sometimes, we want to [[https://docs.pytest.org/en/8.0.x/how-to/skipping.html#skipping-test-functions][skip a test entirely]]. For example, this could be because a certain library is not installed. For this, either use the ~@pytest.mark.skip~ decorator or the ~pytest.skip~ function. The latter can be useful if the condition for skipping is only known at runtime.
#+begin_src python
# code/test_skip.py
def is_peft_installed():
    try:
        import peft
        return True
    except ImportError:
        return False

@pytest.mark.skip(reason="peft is not installed")
def test_peft():
    from peft import get_peft_model

    assert callable(get_peft_model)

def test_peft_2():
    if not is_peft_installed():
        pytest.skip("peft is not installed")

    from peft import get_peft_model
    assert callable(get_peft_model)
#+end_src
Skipped tests are symbolized by a yellow ~s~ instead of a green ~.~ by the test runner. Same as with ~xfail~, it's possible to add a ~reason~ argument to explain why the test is skipped.
** More skipping options
To skip tests only under certain conditions, use the ~pytest.mark.skipif~ decorator. Similar to using ~xfail~ with the ~condition~ argument, this is useful when you want to skip a test only on a specific platform or with a specific dependency version.

Moreover, you can use [[https://docs.pytest.org/en/8.0.x/how-to/skipping.html#id1][pytest.mark.importorskip]] to skip a test if a certain library is not installed. This is useful when you want to skip a test only if a certain library is not installed, but you don't want to skip the entire test suite if the library is not installed.
#+begin_src python
# code/test_skip.py
@pytest.mark.skipif(sys.platform == "win32", reason="This test fails on Windows")
def test_fail_on_windows():
    assert os.path.exists("/tmp")

def test_spacy():
    spacy = pytest.importorskip("spacy")
    nlp = spacy.load("en_core_web_sm")
    assert nlp("This is a test").ents
#+end_src
*Tip*: When you use ~importskip~ on the root of the test file, the whole file will be skipped if the library is not installed. This is useful when you have a test file that tests a library that is not installed on all systems.
** Custom markers
It's possible to define [[https://docs.pytest.org/en/8.0.x/reference/reference.html#custom-marks][arbitrary custom markers]] in pytest. A common use case for this is to make it easy to run only a specific subset of tests when invoking ~pytest~.
#+begin_src python
# code/test_custom_marker.py
@pytest.fixture(scope="module")
def model():
    return AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

@pytest.mark.bloomz
def test_config(model):
    assert model.config.architectures == ["BloomForCausalLM"]

@pytest.mark.bloomz
class TestBloomz:
    def test_num_layers(self, model):
        assert len(model.transformer.h) == 24

    def test_in_features(self, model):
        assert model.transformer.h[0].self_attention.query_key_value.in_features == 1024
#+end_src
In this example, we see that the marker can be applied to both test functions and test classes. In addition to this, we need to add an entry to our ~pyproject.toml~ to let ~pytest~ know about our custom marker:
#+begin_src toml
[tool.pytest.ini_options]
markers = ["bloomz"]
#+end_src
Now, when we run ~pytest code/ -m bloomz~, only the tests marked with ~@pytest.mark.bloomz~ will be run.
** Slow marker
One of the most common use cases for custom markers is to mark slow tests. This is useful when you want to run only the fast tests while developing. There is no difference to other custom markers, as shown in the previous example. However, it is often nice to add a special flag to the test runner to run only the fast tests. This can be achieved by adding an entry to the ~conftest.py~ file in your test folder:
#+begin_src python
def pytest_addoption(parser):
    parser.addoption(
        "--runslow", action="store_true", default=False, help="run slow tests"
    )

def pytest_collection_modifyitems(config, items):
    if config.getoption("--runslow"):
        # --runslow given in cli: do not skip slow tests
        return

    skip_slow = pytest.mark.skip(reason="need --runslow option to run")
    for item in items:
        if "slow" in item.keywords:
            item.add_marker(skip_slow)
#+end_src
Then, inside your actual test function, just add the ~@pytest.mark.slow~ decorator to mark the test as slow:
#+begin_src python
# code/test_custom_marker.py

# this test is slow and skipped by default
@pytest.mark.slow
def test_train_model():
    model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")
    for _ in range(1000):
        # training code
        pass
#+end_src
Of course, also add an entry to ~pyproject.toml~ as explained earlier. With this setup, invoking ~pytest code/~ will skip the marked test, but ~pytest code/ --runslow~ will run it.

*Note*: When adding such a marker, ensure that the new flag is added to the CI scripts as well, or else these tests will be skipped on CI.
* Pytest test runner
** Introduction
We're already using ~pytest~ as a test runner, since it works with ~unittest~ and has a couple of useful features. Still, we'll highlight a few features below that are useful to know about.

With all the features explained in the following section, we assume that ~pytest~ is called from the command line. If you're using an editor or IDE to run your tests, check the available options there to select a subset of tests. Probably, everything that is described below can also be achieved from the editor or IDE, possibly requiring a plugin or extension.
** Selectively running tests
Our test suites are often very large and can take a lot of time to finish. Therefore, we might tend to not running the tests at all on our development machines and instead rely solely on CI. This, however, leads to a slow feedback loop. It is thus important to be able to run only a subset of tests. ~pytest~ has a couple of options to achieve this.

The most common way to run only a subset of tests is to specify the path to the test file or directory. For example, ~pytest code/test_parametrize.py~ will run only the tests in the file ~test_parametrize.py~. You can also specify a directory, in which case all tests in the directory and its subdirectories will be run.

Another common way to run only a subset of tests is to use the ~-k~ option. This option takes a string that is matched against the test names. Only the tests whose names match the string will be run. For example, ~pytest code/ -k forward~ will run only the tests whose names contain the substring "forward". The argument passed to ~-k~ can contain logical operators to refine the filter even more. For instance, run ~pytest code/ -k "TestModel and not layers"~ to run only the tests whose names contain "TestModel" (which is the class name in this case) but not "layers" (which is the method name in this case).

When you're working on a test and you want to run only the tests that failed last time, you can use the ~--lf~ option. This can be especially useful when the tests are slow and you want to skip running the tests that would pass anyway.

If these options are not enough to filter the specific tests you want to run, you should consider adding a marker, as explained in an earlier section. This allows you to mark arbitrary tests and to run only those by invoking ~pytest code/ -m <MARKER-NAME>~.
** Early exit
When running tests, it is often useful to stop the test run as soon as the first test fails. This can be achieved by passing the ~-x~ option to ~pytest~. For example, running ~pytest code/test_unittest_assert.py -x~ will stop the test run as soon as the first test fails. This is also useful when tests take some time to finish.
** Dropping into the debugger
By calling ~pytest~ with the ~--pdb~ option, you can drop into the debugger when a test fails. This is useful when you want to inspect the state of the program at that stage. For example, running ~pytest code/test_unittest_assert.py --pdb~ will drop into the debugger when a test fails. As usual, use the normal debugger commands like ~n~ to step to the next line, ~c~ to continue, ~l~ to list the code, etc.

*Tip*: When using the ~--pdb~ option, it is often useful to also pass the ~-x~ option to stop the test run as soon as the first test fails. Otherwise, if 100 tests fail, you might get stuck in the debugger for a long time.
** More verbose output
By default, ~pytest~ only prints the names of the tests that fail. If you want to see more details, pass the ~-v~ flag to make the output more verbose.

If you want to see the print output of the tests that pass, you can pass the ~-s~ flag. Otherwise, everything that is printed to stdout and stderr is captured and only shown when the test fails.

To see which tests were slowest, pass the ~--durations <N>~ option, where ~N~ is a number. This will print the ~N~ slowest tests at the end of the test run. This can be useful to identify slow tests and to optimize them.
* Plugins
** Test coverage
To ensure that as much code as possible is covered by tests, take a look at [[https://pypi.org/project/pytest-cov/][pytest-cov]]. This plugin will record the line coverage of the tests and print a report at the end, e.g. something like this:
#+begin_src text
Name                                                  Stmts   Miss  Cover   Missing
-----------------------------------------------------------------------------------
src/peft/__init__.py                                      8      0   100%
src/peft/auto.py                                         68     31    54%   52, 72-126
src/peft/config.py                                      101     56    45%   47, 60-80, 95-118, 131-151, 162-165, 169-181, 189-206, 218, 270
src/peft/helpers.py                                      28     28     0%   1-113
#+end_src
This way, it is easy to identify which parts of the code base are covered by tests and which parts aren't. Of course, trying to achieve 100% coverage is not useful most of the time and when you're running only a subset of tests, as described in the previous section, the coverage report will only show the coverage of the tests that were run. Still, this is useful to have, especially when new features are being added. Checking the coverage report, we can ensure that the new code is covered by tests.

When it comes to configuration, consider adding this to your ~pyproject.toml~:
#+begin_src toml
[tool.pytest.ini_options]
addopts = "--cov=src/<package-name> --cov-report=term-missing"
#+end_src
** Pretty summary
This is just a small cosmetic addition, but by installing [[https://pypi.org/project/pytest-pretty/][pytest-pretty]], you will automatically get a nice summary of the test run at the end. This can be useful to quickly see how many tests passed, failed, and were skipped. Here is an example output:
#+begin_src text
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃  File                          ┃  Function                             ┃  Function Line  ┃  Error Line  ┃  Error           ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│  code/test_unittest_assert.py  │  TestUnittestAssert.test_upper        │  6              │  7           │  AssertionError  │
│  code/test_unittest_assert.py  │  TestUnittestAssertMethod.test_upper  │  11             │  12          │  AssertionError  │
│  code/test_unittest_assert.py  │  TestPytestAssert.test_upper          │  16             │  17          │  AssertionError  │
└────────────────────────────────┴───────────────────────────────────────┴─────────────────┴──────────────┴──────────────────┘
#+end_src
** Running tests in parallel
When you have a large test suite, it can be useful to run the tests in parallel to speed up the test run. This requires the [[https://pypi.org/project/pytest-xdist/][pytest-xdist]] plugin. To install it, run ~pip install pytest-xdist~. Then, you can run ~pytest code/ -n <N>~, where ~N~ is the number of workers. This will run the tests in ~N~ parallel processes.
* Special considerations for PyTorch and Hugging Face
** Checking the equality of PyTorch tensors
PyTorch has a dedicated [[https://pytorch.org/docs/stable/testing.html#module-torch.testing][testing module]]. The notable function in this module is ~torch.testing.assert_close~. This function checks for approximate equality, which is important for floating point numbers. Moreover, it will report back the magnitude of the error in case the test fails. Here is an example:

#+begin_src python
# code/test_pytorch.py
def test_forward():
    model = torch.nn.Linear(3, 5)
    x = torch.randn(10, 3)
    y = model(x)
    expected = torch.randn(10, 5)
    torch.testing.assert_allclose(y, expected)
#+end_src

Moreover, it also checks other things, like the device and the dtype of the tensors:

#+begin_src python
# code/test_pytorch.py
def test_other_device():
    x = torch.zeros(10, 3)
    y = torch.zeros(10, 3).to("cuda")
    torch.testing.assert_allclose(x, y)
#+end_src

Finally, when testing PyTorch, it is important that the random seed is set to a fixed value. This can be achieved by adding the following fixture to your test file:

#+begin_src python
@pytest.fixture(autouse=True)
def set_random_seed():
    torch.manual_seed(0)
    np.random.seed(0)

@pytest.mark.usefixtures("set_random_seed")
def test_rand():
    x = torch.rand(2, 2)
    expected = torch.tensor([[0.4963, 0.7682], [0.0885, 0.1320]])
    torch.testing.assert_allclose(x, expected, rtol=1e-4, atol=1e-4)
#+end_src

** Checking the same pretrained models repeatedly
In Hugging Face tests, it is very common to perform different tests on the same pretrained model. This often results in the model being loaded multiple times, which is slow, even if the weights are cached on disk. To avoid this, use a fixture with a scope of ~"session"~. This fixture can be defined in ~conftest.py~ and then will be available automatically in all test files. Here is an example:

#+begin_src python
# code/conftest.py
import pytest

@pytest.fixture(scope="session")
def bloomz_model():
    return AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

# code/test_hf.py
def test_forward(bloomz_model):
    x = torch.zeros(1, 10, dtype=torch.long)
    bloomz_model(x)
#+end_src

Be careful, however, not to mutate the model in any of the tests, as this will affect other tests that use the model fixture. For instance, if you train the model, all subsequent tests that use the model fixture will use the trained model. If you write a test that mutates the model, don't use the global fixture but load a separate model instead, or create a copy of the model.

Also, be aware that if the fixture is session scoped, it will be held in memory for the entire test run. Therefore, use this approach only for small models.
** Freeing up memory
When working with large models, it is important to free up memory after the test is done. To achieve this, use ~yield~ in the fixture, as explained earlier:

#+begin_src python
# code/test_hf.py
@pytest.fixture
def big_model():
    # This model is loaded into memory and then deleted again for each test that
    # uses this fixture
    model = AutoModelForCausalLM.from_pretrained("facebook/opt-125m")
    print("Loaded model into memory")
    yield model
    # clean up after the fixture
    del model
    gc.collect()
    torch.cuda.empty_cache()
    print("Deleted model from memory")

def test_opt_forward(big_model):
    x = torch.zeros(1, 10, dtype=torch.long)
    big_model(x)

def test_opt_generate(big_model):
    big_model.generate(
        torch.zeros(1, 10, dtype=torch.long),
        do_sample=True,
        max_length=20,
        num_return_sequences=1,
    )
#+end_src
* Recommended practices
** Group related tests into classes
Although ~pytest~ does not require to use classes, as it supports tests as standalone functions, it can still often be a good idea to group related tests into classes. A couple of advantages of this approach are:

- It creates a namespace. For example, you could have a fixture called ~model~ in one test class that loads GPT2 and another fixture called ~model~ in another test class that loads T5. If the fixtures were defined on the module level, they would have to have different names. This can quickly become unwieldy as the number of fixtures grows.
- It allows you to decorate the whole class. For example, if all tests in that class require a GPU, you can add the ~@pytest.mark.skipif~ decorator to the class and all its tests will be skipped. If the tests were defined on the module level, you would have to decorate each single test.
- Often, when working on a new feature or bugfix, you'll want to run only the tests related to that feature or bugfix. By grouping related tests into classes, you can use the ~pytest tests/ -k <NAME-OF-CLASS>~ to run exactly those tests in this class. In theory, you might be able to achieve the same by clever use of logic for the ~-k~ option, but this can get complicated quickly. You could also use markers, but unless you plan to run these tests very often, it's overkill.
- Finally, when migrating from ~unittest~ to ~pytest~, it is often easier to keep the same structure. This makes the migration less error-prone and easier to review as it reduces diff noise.

Of course, there are also advantages for using standalone functions. For example, if the class would have only a single associated test method, using a class is unnecessary. Functions are easier to write and require one less level of indentation. Therefore, it is important to find a good balance between the two.
** Document tests
/This section is not specific to ~pytest~/

At the time when we write tests, it is often clear to use what the test is supposed to do. But as with regular code, other contributors might not understand what the test does in detail, or we ourselves might forget the purpose of the test after some time. Therefore, it is important to document the tests when they are non-trivial.

As an example, consider referencing the GitHub issue when a test exists to verify a bugfix. Although this information could also be discovered by looking at the git history, spelling it out saves time. Also, take the time to explain what tests do if the test checks for something that is not obvious. Good naming can also help here: Since test functions are not called directly, it is often better to have a long descriptive name than a short and concise one.

Another thing to consider is to explain "magic numbers". For example, if the test asserts that a certain output should be ~42~, explain where that number comes from.
** Test only one thing at a time
Our tests will often test multiple things at once. For example, here is a test from the PEFT code base:
#+begin_src python
# https://github.com/huggingface/peft/blob/f81147268efb368d2bd0dc280fd651b8107aff3a/tests/test_common_gpu.py#L442
    def test_lora_seq2seq_lm_multi_gpu_inference(self):
        lora_config = LoraConfig(
            r=16, lora_alpha=32, target_modules=["q", "v"], lora_dropout=0.05, bias="none", task_type="SEQ_2_SEQ_LM"
        )

        model = AutoModelForSeq2SeqLM.from_pretrained(self.seq2seq_model_id, device_map="balanced", load_in_8bit=True)
        tokenizer = AutoTokenizer.from_pretrained(self.seq2seq_model_id)

        assert set(model.hf_device_map.values()) == set(range(torch.cuda.device_count()))

        model = get_peft_model(model, lora_config)
        assert isinstance(model, PeftModel)
        assert isinstance(model.base_model.model.encoder.block[0].layer[0].SelfAttention.q, LoraLinear8bitLt)

        dummy_input = "This is a dummy input:"
        input_ids = tokenizer(dummy_input, return_tensors="pt").input_ids.to(self.device)

        # this should work without any problem
        _ = model.generate(input_ids=input_ids)
#+end_src
Notice how the test checks the ~device_map~, the model type, the layer type, and that generation works. This is not ideal, as it makes it harder to understand the purpose of the test. Moreover, if the first assertion fails, the other assertions will not even run. This makes it harder to figure out what exactly went wrong when we see that this test fails on CI, and may require us to comment out the failing assertion to check what happens next.

One of the reasons why the test was written like this is probably that splitting it into separate tests would be slow, as the model would have to be loaded multiple times. However, as we have seen in the previous section on fixtures, this can be easily avoided by using appropriately named fixtures.
** A real world example of improving a test by using ~pytest~ features
TODO
** What to test -- and what not to test
/This section is not specific to ~pytest~/

Many words have been written on this topic, we cannot cover all of that here. However, based on some tests found in the wild, here are some tips:

- Ensure that all the edge cases are covered. This includes running the test on different devices, if applicable, or with different types of models. With ~pytest~, this can often be achieved without too much extra work by using ~@pytest.mark.parametrize~. However, don't forget to also test the happy path, which often involves using the default values for the parameters.
- Test the error handling. This is often forgotten, but it is important to test that the function or class raises the correct error (including error message) in the right circumstances.
- Test the behavior and not implementation details. Put yourself in the shoes of a user and think about how they might use a particular function or class. There is no point in testing that, say, certain private attributes are set correctly, as this is an implementation detail that is prone to change anyway. Ideally, if the implementation changes, the tests should not have to change. If you find that changing the implementation requires changing the tests, either the tests are testing the wrong thing or you're actually breaking backwards compatibility.
** Tips for speeding up the tests
- Use appropriately scoped fixtures to avoid code duplication
- Avoid expensive operations in the module root code, like importing slow libraries that are only used by a few tests. Instead, put them inside of fixtures. That way, they are only imported when the fixture is actually used.
** Have fun writing tests
One of the reasons we skip writing tests is often that it's just not that much fun. This can often lead to more work down the line because we're missing bugs or because a feature is incomplete. Here are some tips to make testing more fun:

- Tests should run quickly. If you have to wait a long time for them to finish, you're probably not going to run them and will be reluctant to add more. With the tips in this guide, especially the section on fixtures, you can often speed up the tests significantly. Using clever command line invocations (or editor settings), you can also ensure that only the necessary tests run. Don't rely solely on CI for running tests if possible.
- Write the tests first. This is often called "test-driven development" and is a good way to ensure good test coverage. You can dedicate time to thinking just about what the tests should cover. And since you'll get quick feedback on whether the tests pass, you'll be more motivated to write them, chasing the oh so desirable ✅.
- Have a hacker mindset. Writing tests is a bit like hacking. You're trying to break your own code. Find all the creative ways in which your code can fail and write tests for them.
